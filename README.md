# ğŸ§  Curriculum Learning em Ambientes de Reinforcement Learning

## ğŸ“Œ Objetivo do Projeto

Este projeto tem como objetivo investigar os efeitos da introduÃ§Ã£o de **Curriculum Learning (CL)** no processo de aprendizado por reforÃ§o. A proposta central Ã© comparar o desempenho de agentes treinados com e sem o uso de um currÃ­culo progressivo de tarefas, avaliando o impacto sobre mÃ©tricas relevantes como:

- â±ï¸ **Tempo de convergÃªncia**
- ğŸ¯ **Valor da recompensa na convergÃªncia**
- ğŸ² **Entropia da polÃ­tica durante o treinamento**

## ğŸ“ O que Ã© Curriculum Learning?

Curriculum Learning Ã© uma estratÃ©gia de treinamento inspirada na forma como humanos aprendem: comeÃ§ando por tarefas simples e progredindo para tarefas mais complexas. No contexto de RL, isso pode significar comeÃ§ar com versÃµes mais fÃ¡ceis de um ambiente e gradualmente aumentar sua complexidade Ã  medida que o agente melhora.

## ğŸ”¬ Metodologia

1. **SeleÃ§Ã£o de Ambientes**  
   Vamos selecionar os seguintes ambientes clÃ¡ssicos:
    - LunarLander-v3
    - BipedalWalker-v3

2. **DefiniÃ§Ã£o dos CurrÃ­culos**  
   A investigaÃ§Ã£o serÃ¡ feita colocando em paralelo o treinamento baseline (sem currÃ­culo) e o treinamento com currÃ­culo progressivo. Para tanto, vamos seguir o seguinte workflow:

   ![Workflow de Treinamento](imgs/workflow.png)

### LunarLander-v3

Para o ambiente LunarLander-v3, o currÃ­culo serÃ¡ definido da seguinte forma:
```python
lunarlander_stages = [
    {
        "name": "easy",
        "gravity": -5.0,
        "enable_wind": False,
        "wind_power": 0.0,
        "turbulence_power": 0.0,
        "parcial_timesteps": 500_000,
    },
    {
        "name": "intermediate",
        "gravity": -10.0,
        "enable_wind": False,
        "wind_power": 0.0,
        "turbulence_power": 0.0,
        "parcial_timesteps": 500_000,
    },
    {
        "name": "hard_with_wind",
        "gravity": -10.0,
        "enable_wind": True,
        "wind_power": 5.0,
        "turbulence_power": 0.5,
        "parcial_timesteps": 500_000,
    },
    {
        "name": "full_difficulty",
        "gravity": -10.0,
        "enable_wind": True,
        "wind_power": 15.0,
        "turbulence_power": 1.5,
        "parcial_timesteps": 500_000,
    }
]
```
### BipedalWalker-v3

Para o ambiente BipedalWalker-v3, o currÃ­culo serÃ¡ definido da seguinte forma:
```python
bipedal_stages = [
    {
        "name": "easy",
        "hardcore": False,
        "parcial_timesteps": 1_000_000,
    },
    {
        "name": "hardcore",
        "hardcore": True,
        "parcial_timesteps": 1_000_000,
    },
]
```

## ğŸ“ˆ MÃ©tricas Avaliadas

- **Tempo de convergÃªncia**: NÃºmero de episÃ³dios atÃ© estabilizaÃ§Ã£o da recompensa mÃ©dia.
- **Recompensa final**: Valor mÃ©dio da recompensa nas Ãºltimas N iteraÃ§Ãµes.
- **Entropia da polÃ­tica**: AvaliaÃ§Ã£o da aleatoriedade das decisÃµes ao longo do tempo (quanto menor, mais determinÃ­stica a polÃ­tica).

## ğŸ§ª Resultados Esperados

A hipÃ³tese Ã© que o uso de curriculum learning:
- ReduzirÃ¡ o tempo de convergÃªncia,
- PermitirÃ¡ alcanÃ§ar recompensas mais altas (melhor polÃ­tica final),
- PromoverÃ¡ polÃ­ticas menos aleatÃ³rias, com entropia decrescente de forma mais estÃ¡vel.

## ğŸ§ª Resultados Obtidos

### Tempo de convergÃªncia

NÃ£o foi afetado pelo uso de curriculum learning, para mais detalhes veja o relatÃ³rio completo no PDF disponÃ­vel na pasta `docs`.

### Recompensa final

O uso de curriculum learning nÃ£o trouxe resultados significativos, para mais detalhes veja o relatÃ³rio completo no PDF disponÃ­vel na pasta `docs`.

### Entropia da polÃ­tica

O uso de curriculum learning para o caso do BipedalWalker-v3 trouxe resultador coerentes com a hipÃ³tese inicial. Para mais detalhes veja o relatÃ³rio completo no PDF disponÃ­vel na pasta `docs`.

## ğŸ“„ CÃ³digos e artefatos

VocÃª pode encontrar os cÃ³digos de execuÃ§Ã£o nos notebooks disponÃ­veis na raiz do diretÃ³rio. AlÃ©m disso, Ã© possÃ­vel encontrar os artefatos gerados durante a execuÃ§Ã£o dos experimentos nas pastas `results` e `saved_models`. 

## ğŸ“š Notebooks de inferÃªncia

Por fim, vocÃª pode usar o notebook `inference.ipynb` para realizar inferÃªncias com os modelos treinados. Basta executar o notebook e seguir as instruÃ§Ãµes para carregar os modelos e realizar inferÃªncias nos ambientes selecionados.